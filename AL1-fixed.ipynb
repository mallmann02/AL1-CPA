{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Laboratório 1\n",
    "### Members\n",
    "- Leonardo Mallmann   21104259\n",
    "- Caetano Muller      21109037\n",
    "- Euzébio Hensel      \n",
    "\n",
    "\n",
    "### PLEASE RUN EACH CELL AT A TIME NOT ALTOGETHER!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, user_agent='wswp', charset='utf-8'):\n",
    "  #print('Downloading:', url)\n",
    "  request = urllib.request.Request(url)\n",
    "  request.add_header('User-agent', user_agent)\n",
    "  try:\n",
    "    resp = urllib.request.urlopen(request)\n",
    "    cs = resp.headers.get_content_charset()\n",
    "    if not cs:\n",
    "        cs = charset\n",
    "    html = resp.read().decode(cs)\n",
    "  except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "    print('Download error:', e.reason)\n",
    "  return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "The following cell runs the code which collects the data from the countries webtsite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(html_content):\n",
    "  soup = BeautifulSoup(html_content, 'html5lib')\n",
    "  rows = soup.select('table tbody tr')\n",
    "  tiempo = datetime.now()\n",
    "  field_names = []\n",
    "  field_values = []\n",
    "  for row in rows:\n",
    "    c = row.select('.w2p_fl label');\n",
    "    field_names.append(c[0].string.strip().replace(':', ''));\n",
    "    v = row.select('.w2p_fw');\n",
    "    if not v[0].string:\n",
    "      if len(re.findall('<img', str(v[0]))) != 0:\n",
    "        img_src = v[0].select('img')[0];\n",
    "        field_values.append(img_src['src'])\n",
    "        continue\n",
    "      if len(re.findall('<div', str(v[0]))) != 0:\n",
    "        a_links = v[0].select('div a');\n",
    "        neighbours = [a_tag.string.strip() for a_tag in a_links]\n",
    "        field_values.append(neighbours);\n",
    "        continue\n",
    "      field_values.append('None')\n",
    "      continue\n",
    "    if v[0].string == ' ':\n",
    "      field_values.append('None')\n",
    "      continue\n",
    "    field_values.append(v[0].string)\n",
    "  field_names.append('Date/Time')\n",
    "  field_values.append(tiempo)\n",
    "  \n",
    "  return field_values, field_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV DATA INSERTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_headers(fieldnames):\n",
    "  with open('places.csv', 'w', newline='\\n') as places_csv:\n",
    "    writer = csv.DictWriter(places_csv, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "def insert_data(data_row, fieldnames):\n",
    "  with open('places.csv', 'a', newline='\\n') as places_csv:\n",
    "    writer = csv.DictWriter(places_csv, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writerow({\n",
    "      f_name: f_value\n",
    "      for f_name, f_value in zip(fieldnames,data_row)\n",
    "    }) # or dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_sitemap(url):\n",
    "  sitemap = download(url)\n",
    "  links = re.findall('\\d(\\/.*)<\\/loc>', sitemap)\n",
    "  for index, link in enumerate(links):\n",
    "    html = download(f'http://localhost:8000{link}')\n",
    "    f_values, f_names = scrap(html)\n",
    "    if index==0:\n",
    "      insert_headers(f_names)\n",
    "    insert_data(f_values, f_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLACE THE URL IN crawl_sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_sitemap('http://localhost:8000/places/static/sitemap.xml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_monitor(url, csv_path):\n",
    "    sitemap = download(url)\n",
    "    lcsv = list(csv.reader(open(csv_path)))\n",
    "    links = re.findall('\\d(\\/.*)<\\/loc>', sitemap)\n",
    "    page_number = 1;\n",
    "    for index, link in enumerate(links):\n",
    "        html = download(f'http://localhost:8000{link}')\n",
    "        f_values, f_names = scrap(html)\n",
    "        for j, value in enumerate(f_values):\n",
    "            if j!=len(f_values)-1:\n",
    "                if lcsv[page_number][j] != f_values[j]:\n",
    "                    if isinstance(f_values[j], list):\n",
    "                      if lcsv[page_number][j] != str(f_values[j]):\n",
    "                        lcsv[page_number][j] = f_values[j]\n",
    "                        lcsv[page_number][len(f_values)-1] = datetime.now()\n",
    "                        continue\n",
    "                      continue\n",
    "                    lcsv[page_number][j] = f_values[j]\n",
    "                    lcsv[page_number][len(f_values)-1] = datetime.now()\n",
    "        if index==2:\n",
    "            break\n",
    "        page_number+=1\n",
    "    writer = csv.writer(open(csv_path, 'w'))\n",
    "    writer.writerows(lcsv)\n",
    "        \n",
    "    \n",
    "page_monitor('http://localhost:8000/places/static/sitemap.xml', 'places.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "# How to execute:\n",
    "\n",
    "Literally just run the import cell and the ones mentioned previously\n",
    "\n",
    "this one consists of basically our methods of scrapping the content from the website\n",
    "with the function imdb_scrap() being called by run_imdb(), this one will call the cJSON()\n",
    "function givimg the dictionary list collected as a parameter to then and only then\n",
    "create the JSON file\n",
    "\n",
    "the imdb top 250 movies website can be found [here](https://www.imdb.com/chart/top/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cJSON(dl):\n",
    "  with open('top_250_movies.json', 'w') as f:\n",
    "    json.dump(dl, f)\n",
    "\n",
    "  return 0\n",
    "  \n",
    "\n",
    "def imdb_scrap(html_content):\n",
    "  soup = BeautifulSoup(html_content, 'html5lib')\n",
    "  rows = soup.select('table tbody tr')\n",
    "  dl = []\n",
    "  count=0\n",
    "\n",
    "  for index,row in enumerate(rows):\n",
    "    count+=1\n",
    "    #getting the posters class\n",
    "    poster_a = row.select('.posterColumn a')\n",
    "    #selecting image class\n",
    "    img=poster_a[0].select('img')\n",
    "    #getting both the redirection url and img source\n",
    "    poster_url = poster_a[0]['href']\n",
    "    poster_img = img[0]['src']\n",
    "    #class of the title year and such...\n",
    "    title_class = row.select('.titleColumn a')\n",
    "    movie_title = title_class[0].string\n",
    "    year_class = row.select('.titleColumn span')\n",
    "    movie_year = year_class[0].string.replace('(', '').replace(')', '')\n",
    "    rating_class = row.select('.ratingColumn.imdbRating strong')\n",
    "    movie_rating = rating_class[0].string\n",
    "    #specific_page_path = row.select('.titleColumn a')[0]['href']\n",
    "    specific_page_url = f'https://www.imdb.com{poster_url}'\n",
    "    specific_page_html = download(specific_page_url)\n",
    "    selector = BeautifulSoup(specific_page_html, 'html5lib')\n",
    "    movie_directors_tag = selector.select('.sc-fa02f843-0.fjLeDR > ul > li:first-child a')\n",
    "    movie_directors_arr = [director.string for director in movie_directors_tag]\n",
    "    genders_tag = selector.select('.ipc-chip-list.sc-16ede01-4.bMBIRz span')\n",
    "    genders_arr = [gender.string for gender in genders_tag]\n",
    "    popularity_tag = selector.select('.sc-edc76a2-2.geydkP')\n",
    "    popularity = popularity_tag[0].string if popularity_tag else 'None'\n",
    "    field_values = [movie_title, movie_year, poster_url, poster_img, movie_rating, popularity,\n",
    "        movie_directors_arr, genders_arr]\n",
    "    field_names = ['title', 'year', 'poster_url',\n",
    "        'poster_img', 'imdb_rating', 'popularity', 'directors', 'genders']\n",
    "    d = dict(zip(field_names, field_values))\n",
    "    dl.append(d)\n",
    "  return cJSON(dl)\n",
    "\n",
    "def run_imdb(url):\n",
    "  a = download(url)\n",
    "  return imdb_scrap(a)\n",
    "\n",
    "run_imdb('https://www.imdb.com/chart/top/')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
