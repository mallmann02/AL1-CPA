{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Laboratório 1\n",
    "### Integrantes\n",
    "- Leonardo Mallmann\n",
    "- Caetano Muller\n",
    "- Euzébio Hensel\n",
    "\n",
    "## QUESTÃO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 5,
>>>>>>> 6f8f52be7ae5125fb76159efe1130f94de64fd79
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, user_agent='wswp', charset='utf-8'):\n",
    "  print('Downloading:', url)\n",
    "  request = urllib.request.Request(url)\n",
    "  request.add_header('User-agent', user_agent)\n",
    "  try:\n",
    "    resp = urllib.request.urlopen(request)\n",
    "    cs = resp.headers.get_content_charset()\n",
    "    if not cs:\n",
    "        cs = charset\n",
    "    html = resp.read().decode(cs)\n",
    "  except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "    print('Download error:', e.reason)\n",
    "  return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(html_content):\n",
    "  soup = BeautifulSoup(html_content, 'html5lib')\n",
    "  rows = soup.select('table tbody tr')\n",
    "  tiempo = datetime.now()\n",
    "  field_names = []\n",
    "  field_values = []\n",
    "  for row in rows:\n",
    "    c = row.select('.w2p_fl label');\n",
    "    field_names.append(c[0].string.strip().replace(':', ''));\n",
    "    field_names.append('Date/Time')\n",
    "    v = row.select('.w2p_fw');\n",
    "    if not v[0].string:\n",
    "      if len(re.findall('<img', str(v[0]))) != 0:\n",
    "        img_src = v[0].select('img')[0];\n",
    "        field_values.append(img_src['src'])\n",
    "        continue\n",
    "      if len(re.findall('<div', str(v[0]))) != 0:\n",
    "        a_links = v[0].select('div a');\n",
    "        neighbours = [a_tag.string.strip() for a_tag in a_links]\n",
    "        field_values.append(neighbours);\n",
    "        continue\n",
    "      field_values.append('None')\n",
    "      continue\n",
    "    if v[0].string == ' ':\n",
    "      field_values.append('None')\n",
    "      continue\n",
    "    field_values.append(v[0].string)\n",
    "  field_values.append(tiempo)\n",
    "  \n",
    "  return field_values, field_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de inserção no CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_headers(fieldnames):\n",
    "  with open('places.csv', 'w', newline='\\n') as places_csv:\n",
    "    writer = csv.DictWriter(places_csv, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "def insert_data(data_row, fieldnames):\n",
    "  with open('places.csv', 'a', newline='\\n') as places_csv:\n",
    "    writer = csv.DictWriter(places_csv, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writerow({\n",
    "      f_name: f_value\n",
    "      for f_name, f_value in zip(fieldnames,data_row)\n",
    "    }) # or dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('places.csv', 'r', newline='') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for row in reader:\n",
    "    row['Capital'].replace(row['Capital'], 'Lego')\n",
    "    print(row['Capital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_sitemap(url):\n",
    "  sitemap = download(url)\n",
    "  links = re.findall('\\d(\\/.*)<\\/loc>', sitemap)\n",
    "  for index, link in enumerate(links):\n",
    "    html = download(f'http://localhost:8000{link}')\n",
    "    f_values, f_names = scrap(html)\n",
    "    if index==0:\n",
    "      insert_headers(f_names)\n",
    "    insert_data(f_values, f_names)\n",
    "    if index==4:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_sitemap('http://localhost:8000/places/static/sitemap.xml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_monitor(url, csv_path):\n",
    "    sitemap = download(url)\n",
    "    lcsv = list(csv.reader(open(csv_path)))\n",
    "    links = re.findall('\\d(\\/.*)<\\/loc>', sitemap)\n",
    "    for index, link in enumerate(links):\n",
    "        html = download(f'http://localhost:8000{link}')\n",
    "        f_values, f_names = scrap(html)\n",
    "        for i, j in range(f_values), range(f_names):\n",
    "            if i!=0 and j!=range(j)-1:\n",
    "                if lcsv[i][j] != f_values[j]:\n",
    "                    lcsv[i][j] = f_values[j]\n",
    "        if index==2:\n",
    "            break\n",
    "    writer = csv.writer(open(csv_path, 'w'))\n",
    "    writer.writerows(lcsv)\n",
    "        \n",
    "    \n",
    "page_monitor(\"kjsd\", 'places.csv')"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 29,
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTÃO 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
>>>>>>> 6f8f52be7ae5125fb76159efe1130f94de64fd79
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.imdb.com/chart/top/\n",
<<<<<<< HEAD
      "Downloading: https://www.imdb.com/title/tt0111161/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=1a264172-ae11-42e4-8ef7-7fed1973bb8f&pf_rd_r=P7ZVM0BPAZFZV9EJCMGT&pf_rd_s=center-1&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_tt_1\n",
      "['Frank Darabont'] ['Drama'] 62\n"
=======
      "9.2\n",
      "9.2\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "8.9\n",
      "8.9\n",
      "8.9\n",
      "8.8\n",
      "8.8\n",
      "8.8\n",
      "8.8\n",
      "8.7\n",
      "8.7\n",
      "8.7\n",
      "8.7\n",
      "8.7\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.6\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.4\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.2\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.1\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n"
>>>>>>> 6f8f52be7ae5125fb76159efe1130f94de64fd79
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "def get_data_from_specific_page(url):\n",
    "    top_250_html = download(url)\n",
    "    soup = BeautifulSoup(top_250_html, 'html5lib')\n",
    "    movies_rows = soup.select('.lister-list tr')\n",
    "    for index, row in enumerate(movies_rows):\n",
    "        specific_page_path = row.select('.titleColumn a')[0]['href']\n",
    "        specific_page_url = f'https://www.imdb.com{specific_page_path}'\n",
    "        specific_page_html = download(specific_page_url)\n",
    "        selector = BeautifulSoup(specific_page_html, 'html5lib')\n",
    "        movie_directors_tag = selector.select('.sc-fa02f843-0.fjLeDR > ul > li:first-child a')\n",
    "        movie_directors_arr = [director.string for director in movie_directors_tag]\n",
    "        genders_tag = selector.select('.ipc-chip-list.sc-16ede01-4.bMBIRz span')\n",
    "        genders_arr = [gender.string for gender in genders_tag]\n",
    "        popularity = selector.select('.sc-edc76a2-1.gopMqI')[0].string\n",
    "        print(movie_directors_arr, genders_arr, popularity)\n",
    "        if index == 0:\n",
    "            break\n",
    "\n",
    "get_data_from_specific_page('https://www.imdb.com/chart/top/');  "
=======
    "def imdb_scrap(html_content):\n",
    "  soup = BeautifulSoup(html_content, 'html5lib')\n",
    "  rows = soup.select('table tbody tr')\n",
    "  field_names = []\n",
    "  field_values = []\n",
    "\n",
    "  for row in rows:\n",
    "    #getting the posters class\n",
    "    poster_a = row.select('.posterColumn a');\n",
    "    #selecting image class\n",
    "    img=poster_a[0].select('img')\n",
    "    #getting both the redirection url and img source\n",
    "    href = poster_a[0]['href']\n",
    "    imgsrc = img[0]['src']\n",
    "\n",
    "    #class of the title year and such...\n",
    "    title_class = row.select('.titleColumn a')\n",
    "    #print(title_class[0].string)\n",
    "    year_class = row.select('.titleColumn span')\n",
    "    #print(year_class[0].string.replace('(', '').replace(')', ''))\n",
    "    rating_class = row.select('.ratingColumn.imdbRating strong')\n",
    "    print(rating_class[0].string)\n",
    "\n",
    "\n",
    "\n",
    "imdb = download('https://www.imdb.com/chart/top/')\n",
    "imdb_scrap(imdb)\n",
    "\n",
    "#     field_names.append(c[0].string.strip().replace(':', ''));\n",
    "#     field_names.append('Date/Time')\n",
    "#     v = row.select('.w2p_fw');\n",
    "#     if not v[0].string:\n",
    "#       if len(re.findall('<img', str(v[0]))) != 0:\n",
    "#         img_src = v[0].select('img')[0];\n",
    "#         field_values.append(img_src['src'])\n",
    "#         continue\n",
    "#       if len(re.findall('<div', str(v[0]))) != 0:\n",
    "#         a_links = v[0].select('div a');\n",
    "#         neighbours = [a_tag.string.strip() for a_tag in a_links]\n",
    "#         field_values.append(neighbours);\n",
    "#         continue\n",
    "#       field_values.append('None')\n",
    "#       continue\n",
    "#     if v[0].string == ' ':\n",
    "#       field_values.append('None')\n",
    "#       continue\n",
    "#     field_values.append(v[0].string)\n",
    "#   field_values.append(tiempo)\n",
    "  \n",
    "#   return field_values, field_names"
>>>>>>> 6f8f52be7ae5125fb76159efe1130f94de64fd79
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
