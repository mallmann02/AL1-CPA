{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Laboratório 1\n",
    "### Integrantes\n",
    "- Leonardo Mallmann\n",
    "- Caetano Muller\n",
    "- Euzébio Hensel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):\n",
    "  print('Downloading:', url)\n",
    "  request = urllib.request.Request(url)\n",
    "  request.add_header('User-agent', user_agent)\n",
    "  try:\n",
    "    resp = urllib.request.urlopen(request)\n",
    "    cs = resp.headers.get_content_charset()\n",
    "    if not cs:\n",
    "        cs = charset\n",
    "    html = resp.read().decode(cs)\n",
    "  except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "    print('Download error:', e.reason)\n",
    "  return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(html_content):\n",
    "  soup = BeautifulSoup(html_content, 'html5lib')\n",
    "  rows = soup.select('table tbody tr')\n",
    "  field_names = []\n",
    "  field_values = []\n",
    "  for row in rows:\n",
    "    c = row.select('.w2p_fl label');\n",
    "    field_names.append(c[0].string.strip().replace(':', ''));\n",
    "    v = row.select('.w2p_fw');\n",
    "    if not v[0].string:\n",
    "      if len(re.findall('<img', str(v[0]))) != 0:\n",
    "        img_src = v[0].select('img')[0];\n",
    "        field_values.append(img_src['src'])\n",
    "        print(img_src['src'])\n",
    "        continue\n",
    "      if len(re.findall('<div', str(v[0]))) != 0:\n",
    "        a_links = v[0].select('div a');\n",
    "        neighbours = [a_tag.string.strip() for a_tag in a_links]\n",
    "        field_values.append(neighbours);\n",
    "        print(neighbours)\n",
    "        continue\n",
    "    field_values.append(v[0].string)\n",
    "    print(v[0].string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de inserção no CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.csv', 'w', newline='') as csvfile:\n",
    "  fieldnames = ['first_name', 'last_name']\n",
    "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "  ingre = ['Baked', 'Beans']\n",
    "\n",
    "  writer.writeheader()\n",
    "  writer.writerow({name: ing for name, ing in zip(fieldnames,ingre)}) # or dict(zip(keys, values))\n",
    "  writer.writerow({'first_name': 'Lovely', 'last_name': 'Spam'})\n",
    "  writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})\n",
    "\n",
    "with open('names.csv', 'r', newline='') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for row in reader:\n",
    "    print(row['first_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_sitemap(url):\n",
    "  sitemap = download(url)\n",
    "  links = re.findall('\\d(\\/.*)<\\/loc>', sitemap)\n",
    "  for index, link in enumerate(links):\n",
    "    html = download(f'http://localhost:8000{link}')\n",
    "    scrap(html)\n",
    "    if index==0:\n",
    "      break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://localhost:8000/places/static/sitemap.xml\n",
      "Downloading: http://localhost:8000/places/default/view/Afghanistan-1\n",
      "647500 square kilometres\n",
      "29121286\n",
      "AF\n",
      "Afghanistan\n",
      "Kabul\n",
      "AS\n",
      ".af\n",
      "AFN\n",
      "Afghani\n",
      "93\n",
      "None\n",
      "None\n",
      "fa-AF,ps,uz-AF,tk\n",
      "['TM', 'CN', 'IR', 'TJ', 'PK', 'UZ']\n"
     ]
    }
   ],
   "source": [
    "crawl_sitemap('http://localhost:8000/places/static/sitemap.xml');"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
